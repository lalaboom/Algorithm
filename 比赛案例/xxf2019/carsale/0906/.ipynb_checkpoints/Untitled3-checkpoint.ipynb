{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] File b'train_sales_data.csv' does not exist: b'train_sales_data.csv'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-2-ac0b287a6fee>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      7\u001b[0m \u001b[0mwarnings\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfilterwarnings\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'ignore'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      8\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 9\u001b[1;33m \u001b[0mtrain_sales_data\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'train_sales_data.csv'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     10\u001b[0m \u001b[0mtrain_search_data\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'train_search_data.csv'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     11\u001b[0m \u001b[0mtrain_user_reply_data\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'train_user_reply_data.csv'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\Users\\X1Carbon\\Anaconda3\\envs\\TensorFlow\\lib\\site-packages\\pandas\\io\\parsers.py\u001b[0m in \u001b[0;36mparser_f\u001b[1;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, squeeze, prefix, mangle_dupe_cols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, dialect, error_bad_lines, warn_bad_lines, delim_whitespace, low_memory, memory_map, float_precision)\u001b[0m\n\u001b[0;32m    683\u001b[0m         )\n\u001b[0;32m    684\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 685\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0m_read\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    686\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    687\u001b[0m     \u001b[0mparser_f\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__name__\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mname\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\Users\\X1Carbon\\Anaconda3\\envs\\TensorFlow\\lib\\site-packages\\pandas\\io\\parsers.py\u001b[0m in \u001b[0;36m_read\u001b[1;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[0;32m    455\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    456\u001b[0m     \u001b[1;31m# Create the parser.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 457\u001b[1;33m     \u001b[0mparser\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mTextFileReader\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfp_or_buf\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    458\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    459\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mchunksize\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0miterator\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\Users\\X1Carbon\\Anaconda3\\envs\\TensorFlow\\lib\\site-packages\\pandas\\io\\parsers.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[0;32m    893\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"has_index_names\"\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mkwds\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"has_index_names\"\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    894\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 895\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_make_engine\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mengine\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    896\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    897\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mclose\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\Users\\X1Carbon\\Anaconda3\\envs\\TensorFlow\\lib\\site-packages\\pandas\\io\\parsers.py\u001b[0m in \u001b[0;36m_make_engine\u001b[1;34m(self, engine)\u001b[0m\n\u001b[0;32m   1133\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_make_engine\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mengine\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m\"c\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1134\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mengine\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m\"c\"\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1135\u001b[1;33m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_engine\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mCParserWrapper\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mf\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1136\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1137\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mengine\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m\"python\"\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\Users\\X1Carbon\\Anaconda3\\envs\\TensorFlow\\lib\\site-packages\\pandas\\io\\parsers.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, src, **kwds)\u001b[0m\n\u001b[0;32m   1915\u001b[0m         \u001b[0mkwds\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"usecols\"\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0musecols\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1916\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1917\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_reader\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mparsers\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTextReader\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msrc\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1918\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0munnamed_cols\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_reader\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0munnamed_cols\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1919\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mpandas\\_libs\\parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader.__cinit__\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32mpandas\\_libs\\parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader._setup_parser_source\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [Errno 2] File b'train_sales_data.csv' does not exist: b'train_sales_data.csv'"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import lightgbm as lgb\n",
    "from sklearn.metrics import mean_squared_error as mse\n",
    "from tqdm import tqdm, tqdm_notebook\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "train_sales_data = pd.read_csv('train_sales_data.csv')\n",
    "train_search_data = pd.read_csv('train_search_data.csv')\n",
    "train_user_reply_data = pd.read_csv('train_user_reply_data.csv')\n",
    "test = pd.read_csv('evaluation_public.csv')\n",
    "\n",
    "data = pd.merge(train_sales_data, train_search_data, 'left', on=['province', 'adcode', 'model', 'regYear', 'regMonth'])\n",
    "data = pd.merge(data, train_user_reply_data, 'left', on=['model', 'regYear', 'regMonth'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    " \n",
    "\n",
    "\n",
    " \n",
    "# train_sales_data\\train_search_data\\train_user_reply_data  拼接\n",
    "\n",
    " \n",
    "# col, col2, col3 中 ，设1.5倍四分位距之外的数据为异常值，用上下四分位数的均值填充\n",
    "col, col2, col3 = ['popularity', 'carCommentVolum', 'newsReplyVolum']\n",
    "col_per = np.percentile(data[col],(25,75))\n",
    "diff = 1.5*(col_per[1] - col_per[0])\n",
    "col_per_in = (data[col] >= col_per[0] - diff) & (data[col] <= col_per[1] + diff)\n",
    " \n",
    "col_per2 = np.percentile(data[col2],(25,75))\n",
    "diff2 = 1.5*(col_per2[1] - col_per2[0])\n",
    "col_per_in2 = (data[col2] >= col_per2[0] - diff2) & (data[col2] <= col_per2[1] + diff2)\n",
    " \n",
    "col_per3 = np.percentile(data[col3],(25,75))\n",
    "diff3 = 1.5*(col_per3[1] - col_per3[0])\n",
    "col_per_in3 = (data[col3] >= col_per3[0] - diff3) & (data[col3] <= col_per3[1] + diff3)\n",
    " \n",
    "data.loc[~col_per_in, col] = col_per.mean()\n",
    "data.loc[~col_per_in2, col2] = col_per2.mean()\n",
    "data.loc[~col_per_in3, col3] = col_per3.mean()\n",
    " \n",
    "# 统计销量\n",
    "data['bt_ry_mean'] = data.groupby(['bodyType','regYear'])['salesVolume'].transform('mean')\n",
    "data['ad_ry_mean'] = data.groupby(['adcode','regYear'])['salesVolume'].transform('mean')\n",
    "data['md_ry_mean'] = data.groupby(['model','regYear'])['salesVolume'].transform('mean')\n",
    " \n",
    " \n",
    "'''\n",
    "一、lgb预测\n",
    "'''\n",
    "# 测试集并入\n",
    "data = pd.concat([data, test], ignore_index=True)\n",
    "data['label'] = data['salesVolume']\n",
    "data['id'] = data['id'].fillna(0).astype(int)\n",
    "del data['salesVolume'], data['forecastVolum']\n",
    "# 填补测试集的车身类型\n",
    "data['bodyType'] = data['model'].map(train_sales_data.drop_duplicates('model').set_index('model')['bodyType'])\n",
    "# 编码 bodyType、model\n",
    "for i in ['bodyType', 'model']:\n",
    "    data[i] = data[i].map(dict(zip(data[i].unique(), range(data[i].nunique()))))\n",
    "# 距离2016年的时间间隔，月数\n",
    "data['mt'] = (data['regYear'] - 2016) * 12 + data['regMonth']\n",
    " \n",
    "shift_feat = []\n",
    "data['model_adcode'] = data['adcode'] + data['model']\n",
    "data['model_adcode_mt'] = data['model_adcode'] * 100 + data['mt']\n",
    " \n",
    "# 填充测试集特征值\n",
    "for col in ['carCommentVolum','newsReplyVolum','popularity','bt_ry_mean','ad_ry_mean', 'md_ry_mean']:\n",
    "    lgb_col_na = pd.isnull(data[col])\n",
    "    data[col] = data[col].replace(0,1)\n",
    "    data.loc[lgb_col_na,col] = \\\n",
    "    ((((data.loc[(data['regYear'].isin([2017]))&(data['regMonth'].isin([1,2,3,4])), col].values /\n",
    "    data.loc[(data['regYear'].isin([2016]))&(data['regMonth'].isin([1,2,3,4])), col].values)))*\n",
    "    data.loc[(data['regYear'].isin([2017]))&(data['regMonth'].isin([1,2,3,4])), col].values * 1.03).round()\n",
    " \n",
    " \n",
    "# 每年的新年在第几月份\n",
    "data['happyNY'] = 0\n",
    "data.loc[(data['regYear'].isin([2016,2018])&data['regMonth'].isin([2])),'happyNY'] = 1\n",
    "data.loc[(data['regYear'].isin([2017])&data['regMonth'].isin([1])),'happyNY'] = 1\n",
    " \n",
    " \n",
    "# label 下移12个月，则测试集填充上了label\n",
    "for i in [4]:\n",
    "    shift_feat.append('shift_model_adcode_mt_label_{0}'.format(i))\n",
    "    data['model_adcode_mt_{0}'.format(i)] = data['model_adcode_mt'] + i\n",
    "    data_last = data[~data.label.isnull()].set_index('model_adcode_mt_{0}'.format(i))\n",
    "    data['shift_model_adcode_mt_label_{0}'.format(i)] = data['model_adcode_mt'].map(data_last['label'])\n",
    " \n",
    "data.loc[pd.isnull(data['shift_model_adcode_mt_label_4']),'shift_model_adcode_mt_label_4'] = \\\n",
    "((data.loc[(data.regMonth.isin([1,2,3,4]))&(data.regYear.isin([2016])),'label'].values/\n",
    " data.loc[(data.regMonth.isin([1,2,3,4]))&(data.regYear.isin([2017])),'label'].values)*\n",
    "data.loc[(data.regMonth.isin([1,2,3,4]))&(data.regYear.isin([2016])),'label'].values).round()\n",
    " \n",
    "# 根据月份添加权重值\n",
    "a = 6; b = 4\n",
    "data['weightMonth'] = data['regMonth'].map({1:a, 2:a, 3:a, 4:a,\n",
    "                                            5:b, 6:b, 7:b, 8:b, 9:b, 10:b, 11:b, 12:b,})\n",
    " \n",
    " \n",
    " \n",
    "def score(data):\n",
    "    pred = data.groupby(['adcode', 'model'])['pred_label'].agg(lambda x: list(x))\n",
    "    label = data.groupby(['adcode', 'model'])['label'].agg(lambda x: list(x))\n",
    "    label_mean = data.groupby(['adcode', 'model'])['label'].agg(lambda x: np.mean(x))\n",
    "    data_agg = pd.DataFrame()\n",
    "    data_agg['pred_label'] = pred\n",
    "    data_agg['label'] = label\n",
    "    data_agg['label_mean'] = label_mean\n",
    "    nrmse_score = []\n",
    "    for raw in data_agg.values:\n",
    "        nrmse_score.append(mse(raw[0], raw[1]) ** 0.5 / raw[2])\n",
    "    return 1 - np.mean(nrmse_score)\n",
    " \n",
    " \n",
    "df_lgb = pd.DataFrame({'id': test['id']})\n",
    "for col_add in ['ad_ry_mean', 'md_ry_mean', 'bt_ry_mean']:\n",
    "    # 取用的字段，用于训练模型\n",
    "    num_feat = shift_feat\n",
    "    cate_feat = ['adcode', 'bodyType', 'model', 'regYear', 'regMonth', 'happyNY']\n",
    "    features = num_feat + cate_feat + ['popularity', 'carCommentVolum', 'newsReplyVolum', 'weightMonth'] + [col_add]  # [ad_ry_mean, md_ry_mean, bt_ry_mean]\n",
    " \n",
    "    train_idx = (data['mt'] <= 20) # 小于等于20月以内的数据作为训练集\n",
    "    valid_idx = (data['mt'].between(21, 24)) # 21到24个月的数据作为验证集\n",
    "    test_idx = (data['mt'] > 24) # 大于24个月的是测试集\n",
    " \n",
    "    # label\n",
    "    data['model_mean'] = data.groupby('model')['label'].transform('mean') # mean\n",
    "    data['n_label'] = np.log(data['label'])\n",
    " \n",
    "    train_x = data[train_idx][features]\n",
    "    train_y = data[train_idx]['n_label']\n",
    " \n",
    "    valid_x = data[valid_idx][features]\n",
    "    valid_y = data[valid_idx]['n_label']\n",
    " \n",
    "    ############################### lgb ###################################\n",
    "    lgb_model = lgb.LGBMRegressor(\n",
    "        num_leaves=40, reg_alpha=1, reg_lambda=0.1, objective='mse',\n",
    "        max_depth=-1, learning_rate=0.05, min_child_samples=5, random_state=2019,\n",
    "        n_estimators=8000, subsample=0.8, colsample_bytree=0.8)\n",
    " \n",
    "    lgb_model.fit(train_x, train_y, eval_set=[(valid_x, valid_y)],\n",
    "                  categorical_feature=cate_feat, early_stopping_rounds=100, verbose=300)\n",
    "    data['pred_label'] = np.e ** lgb_model.predict(data[features])\n",
    "    model = lgb_model\n",
    "    # 特征重要程度\n",
    "    print ('lgb特征重要程度：',sorted(dict(zip(train_x.columns,model.feature_importances_)).items(),key=lambda x: x[1], reverse=True))\n",
    "    print('NRMSE的均值:',score(data = data[valid_idx]))\n",
    "    model.n_estimators = model.best_iteration_\n",
    "    model.fit(data[~test_idx][features], data[~test_idx]['n_label'], categorical_feature=cate_feat)\n",
    "    data['forecastVolum'] = np.e ** model.predict(data[features])\n",
    "    sub = data[test_idx][['id']]\n",
    "    sub['forecastVolum'] = data[test_idx]['forecastVolum'].apply(lambda x: 0 if x < 0 else x).round().astype(int)\n",
    "    sub_lgb = sub.reset_index(drop=True)\n",
    "    sub_lgb = sub_lgb[['id','forecastVolum']]\n",
    "    print('lgb中forecastVolmn的0值数量：',(sub_lgb['forecastVolum']==0).sum())\n",
    "    df_lgb[col_add] = sub_lgb['forecastVolum']\n",
    "df_lgb.to_csv(path + \"/Result/df_lgb.csv\", index=False) \n",
    "# df_lgb有三列值，任一一列提交，上0.57，祝各位好运！！\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
